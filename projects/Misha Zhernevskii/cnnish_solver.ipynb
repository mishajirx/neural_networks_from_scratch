{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb0d2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MysteryImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.ids = df.iloc[:, 0].values\n",
    "        \n",
    "        if self.is_test:\n",
    "            self.X = df.iloc[:, 1:].values.astype(np.float32)\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.X = df.iloc[:, 1:-1].values.astype(np.float32)\n",
    "            self.y = df.iloc[:, -1].values.astype(np.int64) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features_1d = self.X[idx] # Shape (205,)\n",
    "        \n",
    "        features_padded = np.pad(features_1d, (0, 20), 'constant', constant_values=0)\n",
    "        image = torch.tensor(features_padded).view(1, 15, 15)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            label = torch.tensor(self.y[idx])\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4adfb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from files\n",
    "full_dataset = MysteryImageDataset(\"data/train.csv\", is_test=False)\n",
    "submission_dataset = MysteryImageDataset(\"data/test.csv\", is_test=True)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_data, val_data = random_split(full_dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "755bff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(submission_dataset, batch_size=batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9285ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e57c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "        # BLOCK 1: FINDING EDGES\n",
    "        # (64, 1, 15, 15) -> One grayscale image\n",
    "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1), \n",
    "        # Shape stays (16, 15, 15) because of padding=1 and 3x3 kernel shrinking.\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2), \n",
    "        # New Shape: (16, 7, 7)\n",
    "        # BLOCK 2: FINDING SHAPES\n",
    "        # (16, 7, 7)\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1), \n",
    "        # Shape stays (32, 7, 7)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        # New Shape: (32, 3, 3)\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2) \n",
    "        # Final Shape: (64, 1, 1)\n",
    ")\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(64, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5) # 5 Classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cea44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device().type if torch.cuda.is_available() else \"cpu\"\n",
    "model = SimpleCNN().to(device)\n",
    "learning_rate = 1e-3    \n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf177bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    return running_loss / len(dataloader.dataset), correct / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    return running_loss / len(dataloader.dataset), correct / len(dataloader.dataset)\n",
    "\n",
    "def test_loop(dataloader, model, output_file):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            \n",
    "            logits = model(images)\n",
    "            \n",
    "            \n",
    "            batch_preds = logits.argmax(1).cpu().numpy()\n",
    "            all_preds.extend(batch_preds)\n",
    "            \n",
    "    submission_df = pd.DataFrame({\n",
    "        \"id\": dataloader.dataset.ids, \n",
    "        \"label\": all_preds\n",
    "    })\n",
    "    \n",
    "\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Done! Saved {len(submission_df)} predictions to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3631b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1/30 | Train Loss: 1.2224 | Val Loss: 1.0532 | Val Acc: 62.62%\n",
      "Epoch 2/30 | Train Loss: 0.9388 | Val Loss: 0.9228 | Val Acc: 66.81%\n",
      "Epoch 3/30 | Train Loss: 0.8020 | Val Loss: 0.8360 | Val Acc: 70.50%\n",
      "Epoch 4/30 | Train Loss: 0.6834 | Val Loss: 0.8136 | Val Acc: 71.56%\n",
      "Epoch 5/30 | Train Loss: 0.6181 | Val Loss: 0.8030 | Val Acc: 72.75%\n",
      "Epoch 6/30 | Train Loss: 0.5227 | Val Loss: 0.7786 | Val Acc: 73.12%\n",
      "Epoch 7/30 | Train Loss: 0.4643 | Val Loss: 0.8119 | Val Acc: 74.19%\n",
      "Epoch 8/30 | Train Loss: 0.4025 | Val Loss: 0.7942 | Val Acc: 73.31%\n",
      "Epoch 9/30 | Train Loss: 0.3547 | Val Loss: 0.8948 | Val Acc: 72.38%\n",
      "Epoch 10/30 | Train Loss: 0.3082 | Val Loss: 0.8409 | Val Acc: 74.38%\n",
      "Epoch 11/30 | Train Loss: 0.2680 | Val Loss: 0.8121 | Val Acc: 76.00%\n",
      "Epoch 12/30 | Train Loss: 0.2299 | Val Loss: 0.9345 | Val Acc: 74.50%\n",
      "Epoch 13/30 | Train Loss: 0.1994 | Val Loss: 1.0257 | Val Acc: 72.44%\n",
      "Epoch 14/30 | Train Loss: 0.2213 | Val Loss: 0.9504 | Val Acc: 74.38%\n",
      "Epoch 15/30 | Train Loss: 0.1541 | Val Loss: 0.9869 | Val Acc: 75.38%\n",
      "Epoch 16/30 | Train Loss: 0.1339 | Val Loss: 1.1734 | Val Acc: 75.00%\n",
      "Epoch 17/30 | Train Loss: 0.1217 | Val Loss: 1.2601 | Val Acc: 72.25%\n",
      "Epoch 18/30 | Train Loss: 0.1562 | Val Loss: 1.0486 | Val Acc: 77.25%\n",
      "Epoch 19/30 | Train Loss: 0.0980 | Val Loss: 1.1901 | Val Acc: 76.06%\n",
      "Epoch 20/30 | Train Loss: 0.1447 | Val Loss: 1.2175 | Val Acc: 75.25%\n",
      "Epoch 21/30 | Train Loss: 0.0738 | Val Loss: 1.3847 | Val Acc: 76.88%\n",
      "Epoch 22/30 | Train Loss: 0.1119 | Val Loss: 1.3522 | Val Acc: 71.19%\n",
      "Epoch 23/30 | Train Loss: 0.0878 | Val Loss: 1.2875 | Val Acc: 75.81%\n",
      "Epoch 24/30 | Train Loss: 0.0617 | Val Loss: 1.5486 | Val Acc: 76.12%\n",
      "Epoch 25/30 | Train Loss: 0.0575 | Val Loss: 1.4327 | Val Acc: 76.44%\n",
      "Epoch 26/30 | Train Loss: 0.1033 | Val Loss: 1.5121 | Val Acc: 75.06%\n",
      "Epoch 27/30 | Train Loss: 0.0918 | Val Loss: 1.3568 | Val Acc: 77.50%\n",
      "Epoch 28/30 | Train Loss: 0.0680 | Val Loss: 1.4353 | Val Acc: 76.44%\n",
      "Epoch 29/30 | Train Loss: 0.0574 | Val Loss: 1.5110 | Val Acc: 74.75%\n",
      "Epoch 30/30 | Train Loss: 0.0841 | Val Loss: 1.4749 | Val Acc: 71.88%\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 30\n",
    "print(\"Starting Training...\")\n",
    "history_records = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_dataloader, loss_fn, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_dataloader, loss_fn, device)\n",
    "\n",
    "    record = {\n",
    "        \"record_id\": epoch,              \n",
    "        \"model_id\": 5,\n",
    "        \"training_loss\": train_loss,\n",
    "        \"validation_loss\": val_loss,\n",
    "        \"training_acc\": train_acc,   \n",
    "        \"validation_acc\": val_acc\n",
    "    }\n",
    "\n",
    "    history_records.append(record)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "print(\"Done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 30 records to existing experiment_logs.csv\n",
      "\n",
      "Latest Records:\n",
      "   record_id  model_id  training_loss  validation_loss  training_acc  \\\n",
      "0          0         5       1.222441         1.053217      0.539687   \n",
      "1          1         5       0.938844         0.922842      0.656094   \n",
      "2          2         5       0.801999         0.836041      0.713906   \n",
      "3          3         5       0.683378         0.813614      0.755469   \n",
      "4          4         5       0.618062         0.802962      0.786250   \n",
      "\n",
      "   validation_acc  \n",
      "0        0.626250  \n",
      "1        0.668125  \n",
      "2        0.705000  \n",
      "3        0.715625  \n",
      "4        0.727500  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "output_file = \"experiment_logs.csv\"\n",
    "\n",
    "\n",
    "new_df = pd.DataFrame(history_records)\n",
    "\n",
    "\n",
    "final_df = new_df[['record_id', 'model_id', 'training_loss', 'validation_loss', 'training_acc', 'validation_acc']]\n",
    "\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "\n",
    "    final_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    print(f\"Appended {len(final_df)} records to existing {output_file}\")\n",
    "else:\n",
    "\n",
    "    final_df.to_csv(output_file, mode='w', header=True, index=False)\n",
    "    print(f\"Created new file {output_file} with {len(final_df)} records\")\n",
    "\n",
    "\n",
    "print(\"\\nLatest Records:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a446cf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Saved 2000 predictions to 'submission4.csv'\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, model, \"submission4.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
